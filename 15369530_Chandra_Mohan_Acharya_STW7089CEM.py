# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YfGwYSCxRRHcjzhBMqM7GWSiV5Wf0aL_
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
import statsmodels.api as sm
from scipy import stats

file_path = '/content/drive/MyDrive/gene_data.csv_1718251756549.xlsx'
gene_data = pd.read_excel(file_path)

#--------------------------------------------------------------------------------
# Task 1: Data Exploration and Visualization
#--------------------------------------------------------------------------------


# Task 1.1: Time Series Plots for each gene
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 15))
axes = axes.flatten()

for i, gene in enumerate(gene_data.columns[1:]):
    axes[i].plot(gene_data["Time (min)"], gene_data[gene], label=gene)
    axes[i].set_title(f'Time Series Plot of {gene}')
    axes[i].set_xlabel('Time (min)')
    axes[i].set_ylabel('Expression Level')
    axes[i].legend()

plt.tight_layout()
plt.show()


# Task 1.2: Distribution Plots for each gene
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 15))
axes = axes.flatten()

for i, gene in enumerate(gene_data.columns[1:]):
    sns.histplot(gene_data[gene], kde=True, ax=axes[i])
    axes[i].set_title(f'Distribution of {gene}')
    axes[i].set_xlabel('Expression Level')
    axes[i].set_ylabel('Frequency')

plt.tight_layout()
plt.show()


# Task 1.3: Calculate and Plot the Correlation Matrix
correlation_matrix = gene_data.iloc[:, 1:].corr()  # Exclude the 'Time (min)' column
print("Correlation Matrix:\n", correlation_matrix)

# Plot the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix of Gene Expression Levels')
plt.show()
# Set up the plotting environment
sns.set(style="whitegrid")

# Define the pairs of genes to plot
gene_pairs = [('x1', 'x2'), ('x1', 'x3'), ('x2', 'x4'), ('x3', 'x5'), ('x4', 'x5')]

# Create scatter plots for each pair of genes
plt.figure(figsize=(15, 10))
plot_number = 1

for x_gene, y_gene in gene_pairs:
    plt.subplot(2, 3, plot_number)  # Adjust the number of rows and columns as needed
    sns.scatterplot(x=gene_data[x_gene], y=gene_data[y_gene])
    plt.xlabel(x_gene)
    plt.ylabel(y_gene)
    plt.title(f'Scatter Plot: {x_gene} vs {y_gene}')
    plot_number += 1

plt.tight_layout()
plt.show()

#--------------------------------------------------------------------------------
# Task 2: Model Selection and Evaluation
#--------------------------------------------------------------------------------


y = gene_data['x2'].values
X = gene_data[['x1', 'x3', 'x4', 'x5']].values

# Task 2.1: Create Design Matrices for Each Model

# Model 1: y ~ X4 + X1^2 + X1^3 + X2^4 + X1^4
X_model1 = np.column_stack((np.ones(len(X)), X[:, 2], X[:, 0]**2, X[:, 0]**3, y**4, X[:, 0]**4))
# Model 2: y ~ X4 + X1^3 + X3^4
X_model2 = np.column_stack((np.ones(len(X)), X[:, 2], X[:, 0]**3, X[:, 1]**4))
# Model 3: y ~ X3^3 + X3^4
X_model3 = np.column_stack((np.ones(len(X)), X[:, 1]**3, X[:, 1]**4))
# Model 4: y ~ X2 + X1^3 + X3^4
X_model4 = np.column_stack((np.ones(len(X)), y, X[:, 0]**3, X[:, 1]**4))
# Model 5: y ~ X4 + X1^2 + X1^3 + X3^4
X_model5 = np.column_stack((np.ones(len(X)), X[:, 2], X[:, 0]**2, X[:, 0]**3, X[:, 1]**4))

# Define a function to calculate least squares solution
def least_squares(X, y):
    return np.linalg.inv(X.T @ X) @ X.T @ y

# Calculate parameter estimates (theta_hat) for each model
theta_hat_model1 = least_squares(X_model1, y)
theta_hat_model2 = least_squares(X_model2, y)
theta_hat_model3 = least_squares(X_model3, y)
theta_hat_model4 = least_squares(X_model4, y)
theta_hat_model5 = least_squares(X_model5, y)



# Print the theta_hat for each model
print("Theta_hat for Model 1:", theta_hat_model1)
print("Theta_hat for Model 2:", theta_hat_model2)
print("Theta_hat for Model 3:", theta_hat_model3)
print("Theta_hat for Model 4:", theta_hat_model4)
print("Theta_hat for Model 5:", theta_hat_model5)

# Task 2.2: Calculate Residual Sum of Squares (RSS) for Each Model

# Calculate the predicted Y (Y_hat) for each model
Y_hat_model1 = X_model1 @ theta_hat_model1
Y_hat_model2 = X_model2 @ theta_hat_model2
Y_hat_model3 = X_model3 @ theta_hat_model3
Y_hat_model4 = X_model4 @ theta_hat_model4
Y_hat_model5 = X_model5 @ theta_hat_model5

# Calculate Residual Sum of Squares (RSS) for each model
RSS_model1 = np.sum((y - Y_hat_model1)**2)
RSS_model2 = np.sum((y - Y_hat_model2)**2)
RSS_model3 = np.sum((y - Y_hat_model3)**2)
RSS_model4 = np.sum((y - Y_hat_model4)**2)
RSS_model5 = np.sum((y - Y_hat_model5)**2)

# Print RSS for each model
print("RSS for Model 1:", RSS_model1)
print("RSS for Model 2:", RSS_model2)
print("RSS for Model 3:", RSS_model3)
print("RSS for Model 4:", RSS_model4)
print("RSS for Model 5:", RSS_model5)


# Task 2.3: Calculate Variance and Likelihood for Each Model
n = len(y)  # Number of observations

# Calculate Variance for each model
VAR_model1 = RSS_model1 / (n - 1)
VAR_model2 = RSS_model2 / (n - 1)
VAR_model3 = RSS_model3 / (n - 1)
VAR_model4 = RSS_model4 / (n - 1)
VAR_model5 = RSS_model5 / (n - 1)

# Calculate Likelihood for each model
Likelihood_model1 = -(n/2) * (np.log(2 * np.pi)) - (n/2) * np.log(VAR_model1) - (1/(2 * VAR_model1)) * RSS_model1
Likelihood_model2 = -(n/2) * (np.log(2 * np.pi)) - (n/2) * np.log(VAR_model2) - (1/(2 * VAR_model2)) * RSS_model2
Likelihood_model3 = -(n/2) * (np.log(2 * np.pi)) - (n/2) * np.log(VAR_model3) - (1/(2 * VAR_model3)) * RSS_model3
Likelihood_model4 = -(n/2) * (np.log(2 * np.pi)) - (n/2) * np.log(VAR_model4) - (1/(2 * VAR_model4)) * RSS_model4
Likelihood_model5 = -(n/2) * (np.log(2 * np.pi)) - (n/2) * np.log(VAR_model5) - (1/(2 * VAR_model5)) * RSS_model5

# Task 2.4: Calculate AIC and BIC for Each Model

# Print Variance and Likelihood for each model
print("Variance for Model 1:", VAR_model1)
print("Variance for Model 2:", VAR_model2)
print("Variance for Model 3:", VAR_model3)
print("Variance for Model 4:", VAR_model4)
print("Variance for Model 5:", VAR_model5)

print("Likelihood for Model 1:", Likelihood_model1)
print("Likelihood for Model 2:", Likelihood_model2)
print("Likelihood for Model 3:", Likelihood_model3)
print("Likelihood for Model 4:", Likelihood_model4)
print("Likelihood for Model 5:", Likelihood_model5)

# Calculate AIC for each model
AIC_model1 = 2 * (len(theta_hat_model1)) - 2 * Likelihood_model1
AIC_model2 = 2 * (len(theta_hat_model2)) - 2 * Likelihood_model2
AIC_model3 = 2 * (len(theta_hat_model3)) - 2 * Likelihood_model3
AIC_model4 = 2 * (len(theta_hat_model4)) - 2 * Likelihood_model4
AIC_model5 = 2 * (len(theta_hat_model5)) - 2 * Likelihood_model5

# Calculate BIC for each model
BIC_model1 = len(theta_hat_model1) * np.log(n) - 2 * Likelihood_model1
BIC_model2 = len(theta_hat_model2) * np.log(n) - 2 * Likelihood_model2
BIC_model3 = len(theta_hat_model3) * np.log(n) - 2 * Likelihood_model3
BIC_model4 = len(theta_hat_model4) * np.log(n) - 2 * Likelihood_model4
BIC_model5 = len(theta_hat_model5) * np.log(n) - 2 * Likelihood_model5

# Print AIC and BIC for each model
print("AIC for Model 1:", AIC_model1)
print("AIC for Model 2:", AIC_model2)
print("AIC for Model 3:", AIC_model3)
print("AIC for Model 4:", AIC_model4)
print("AIC for Model 5:", AIC_model5)

print("BIC for Model 1:", BIC_model1)
print("BIC for Model 2:", BIC_model2)
print("BIC for Model 3:", BIC_model3)
print("BIC for Model 4:", BIC_model4)
print("BIC for Model 5:", BIC_model5)

# Task 2.5: Calculating Error for all models and Plotting Q-Q plot with Q-Q line for them
error_model1 = y - Y_hat_model1
error_model2 = y - Y_hat_model2
error_model3 = y - Y_hat_model3
error_model4 = y - Y_hat_model4
error_model5 = y - Y_hat_model5

# # Plotting Q-Q plots
fig, axs = plt.subplots(3, 2, figsize=(15, 10))

sm.qqplot(error_model1, line='45', ax=axs[0, 0])
axs[0, 0].set_title('Q-Q plot of Model 1')

sm.qqplot(error_model2, line='45', ax=axs[0, 1])
axs[0, 1].set_title('Q-Q plot of Model 2')

sm.qqplot(error_model3, line='45', ax=axs[1, 0])
axs[1, 0].set_title('Q-Q plot of Model 3')

sm.qqplot(error_model4, line='45', ax=axs[1, 1])
axs[1, 1].set_title('Q-Q plot of Model 4')

sm.qqplot(error_model5, line='45', ax=axs[2, 0])
axs[2, 0].set_title('Q-Q plot of Model 5')

plt.tight_layout()
plt.show()

# Task 2.7: Splitting input and output data into training and testing sets

# Split the data into training and testing sets (70% for training, 30% for testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Model 2: y ~ X4 + X1^3 + X3^4 (selected as the best model)
# Define the design matrix for Model 2 for training and testing
X_train_model2 = np.column_stack((np.ones(len(X_train)), X_train[:, 2], X_train[:, 0]**3, X_train[:, 1]**4))
X_test_model2 = np.column_stack((np.ones(len(X_test)), X_test[:, 2], X_test[:, 0]**3, X_test[:, 1]**4))

# Fit Model 2 on the training data
theta_hat_model2_train = least_squares(X_train_model2, y_train)

# Predictions on the testing data
y_pred_model2_test = X_test_model2 @ theta_hat_model2_train

# Calculate 95% confidence intervals for predictions
std_dev_model2 = np.std(y_pred_model2_test)
confidence_interval_model2 = 1.96 * std_dev_model2

lower_bound_model2 = y_pred_model2_test - confidence_interval_model2
upper_bound_model2 = y_pred_model2_test + confidence_interval_model2

# Plotting predictions with error bars and actual data
plt.errorbar(range(len(y_test)), y_pred_model2_test, yerr=confidence_interval_model2, fmt='o', label='Prediction with 95% CI')
plt.plot(range(len(y_test)), y_test, 'r-', label='Actual')
plt.xlabel('Samples')
plt.ylabel('Output')
plt.title('Model 2 Predictions with 95% Confidence Intervals')
plt.legend()
plt.show()

#--------------------------------------------------------------------------------
# Task 1: Approximate Bayesian Computation (ABC)
#--------------------------------------------------------------------------------


theta_2, theta_3 = theta_hat_model2[1], theta_hat_model2[2]

# Define Uniform priors around estimated values
theta_2_prior = np.random.uniform(2.5, 4.5, size=10000)
theta_3_prior = np.random.uniform(-2.0, -0.5, size=10000)

# Perform rejection ABC
accepted_theta_2 = []
accepted_theta_3 = []

# Increase the threshold to allow more samples to be accepted
threshold = 500  # Further increasing the threshold

distances = []  # List to store distances for debugging

for i in range(len(theta_2_prior)):
    theta_sample = np.array([theta_hat_model2[0], theta_2_prior[i], theta_3_prior[i], theta_hat_model2[3]])
    y_sample = X_model2 @ theta_sample
    distance = np.sum((y - y_sample)**2)
    distances.append(distance)

    if distance < threshold:  # Acceptable distance threshold
        accepted_theta_2.append(theta_2_prior[i])
        accepted_theta_3.append(theta_3_prior[i])


    # Plot the joint and marginal posterior distributions
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))

    # Joint posterior distribution
    axes[0].scatter(accepted_theta_2, accepted_theta_3, alpha=0.5, color='blue')
    axes[0].set_title('Joint Posterior Distribution')
    axes[0].set_xlabel('Theta 2')
    axes[0].set_ylabel('Theta 3')

    # Marginal posterior distributions
    sns.histplot(accepted_theta_2, kde=True, ax=axes[1], color='blue', label='Theta 2')
    sns.histplot(accepted_theta_3, kde=True, ax=axes[1], color='green', label='Theta 3')
    axes[1].set_title('Marginal Posterior Distributions')
    axes[1].set_xlabel('Parameter Value')
    axes[1].legend()

    plt.tight_layout()
    plt.show()

